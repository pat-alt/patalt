---
title: "Individual recourse for Black Box Models"
subtitle: "Explained through a tale of üê±s and üê∂s"
author: "Patrick Altmeyer"
date: "2021-04-26"
output:
  blogdown::html_page:
      toc: true
bibliography: ../../../bib.bib
link-citations: true
categories: [Data Science, Trustworth AI]
tags: [gradient descent, classification, individual recourse]
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#from-to">From üê± to üê∂</a></li>
<li><a href="#discussion">Discussion</a></li>
<li><a href="#references">References</a></li>
<li><a href="#annex">Annex</a>
<ul>
<li><a href="#revise-simplified"><code>REVISE</code> (simplified)</a></li>
<li><a href="#linear-classifier">Linear classifier</a></li>
</ul></li>
</ul>
</div>

<blockquote>
<p>‚ÄúYou cannot appeal to [algorithms]. They do not listen. Nor do they bend.‚Äù</p>
<p>‚Äî Cathy O‚ÄôNeil</p>
</blockquote>
<p><img src ="www/toy.gif" style="float:right; margin-left: auto; margin-right: auto; width:200px; height:200px;"></p>
<p>In her popular book <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">Weapons of Math Destruction</a> Cathy O‚ÄôNeil presents the example of public school teacher Sarah Wysocki, who lost her job after a teacher evaluation algorithm had rendered her redundant <span class="citation">(<a href="#ref-o2016weapons" role="doc-biblioref">O‚Äôneil 2016</a>)</span>. Sarah was highly popular among her peers, supervisors and students. This post looks at a novel algorithmic solution to the problem that individuals like Sarah who are faced with an undesirable outcome should be provided with means to revise that outcome. The literature commonly refers to this as <em>individual recourse</em>.</p>
<p>One of the first approaches towards individual recourse was proposed by <span class="citation"><a href="#ref-ustun2019actionable" role="doc-biblioref">Ustun, Spangher, and Liu</a> (<a href="#ref-ustun2019actionable" role="doc-biblioref">2019</a>)</span>. In a recent follow-up paper, <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span> propose a methodology coined <code>REVISE</code>, which extends the earlier approach in at least three key ways:</p>
<ol style="list-style-type: decimal">
<li><code>REVISE</code> provides a framework that avoids suggesting an unrealistic set of changes by imposing a threshold likelihood on the revised attributes.</li>
<li>It is applicable to a broader class of models including Black Box classifiers and structural causal models.</li>
<li>It can be used to detect poorly defined proxies and biases.</li>
</ol>
<p>For a detailed discussion of these points you may check out this <a href="paper_presentation.pdf">slide deck</a> or consult the paper directly (freely available on <a href="https://deepai.org/publication/towards-realistic-individual-recourse-and-actionable-explanations-in-black-box-decision-making-systems">DeepAI</a>). Here, we will abstract from some of these complications and instead look at an application of a slightly simplified version of <code>REVISE</code>. This should help you to first build a solid intuition. Readers interested in the technicalities and code may find all of this in the annex below.</p>
<div id="from-to" class="section level2">
<h2>From üê± to üê∂</h2>
<p>We will explain <code>REVISE</code> through a short tale of cats and dogs. The protagonist of our story is Kitty üê±, a young cat that identifies as a dog. Unfortunately though, she is not very tall and her tail, though short for a cat, is longer than that of the average dog (Figure <a href="#fig:density">1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:density"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/density-1.png" alt="Empirical distributions of simulated data set describing cats and dogs. Vertical stalks represent Kitty's attribute values." width="864" />
<p class="caption">
Figure 1: Empirical distributions of simulated data set describing cats and dogs. Vertical stalks represent Kitty‚Äôs attribute values.
</p>
</div>
<p>To her dismay, Kitty has been recognized as a cat by a linear classifier <span class="math inline">\(g_n(X)\)</span> that we trained through stochastic gradient descent (once again interested readers may find technical details and code in the annex below). Figure <a href="#fig:class">2</a> shows the resulting linear separation in the attribute space with the decision boundary in solid black and Kitty‚Äôs location indicated through a red circle. Can we provide individual recourse for Kitty?</p>
<div class="figure" style="text-align: center"><span id="fig:class"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/class-1.png" alt="Linear separation of cats and dogs in the 2-dimensional attribute space with the decision boundary of the fitted classifier in solid black. Kitty's location is indicated through the red circle." width="672" />
<p class="caption">
Figure 2: Linear separation of cats and dogs in the 2-dimensional attribute space with the decision boundary of the fitted classifier in solid black. Kitty‚Äôs location is indicated through the red circle.
</p>
</div>
<p>Let‚Äôs see if and how we can apply <code>REVISE</code> to Kitty‚Äôs problem. The following summary should give you some flavour of how the algorithm works:</p>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\mathbf{x}_i&#39;^{(0)}\)</span> - the attributes that will be revised recursively. Kitty‚Äôs original attributes seem like a reasonable place to start.</li>
<li>Through gradient descent recursively revise <span class="math inline">\(\mathbf{x}_i&#39;^{(t)}\)</span> until <span class="math inline">\(g_n(\mathbf{x}_i&#39;^{(T)})=\)</span>üê∂. At this point <span class="math inline">\(T\)</span> the descent terminates since for these revised attributes the classifier labels Kitty as a dog.</li>
<li>Return <span class="math inline">\(\delta=\mathbf{x}_i&#39;^{(0)}-\mathbf{x}_i\)</span>, that is the individual recourse for Kitty.</li>
</ol>
<p>Figure <a href="#fig:revise">3</a> illustrates what happens when this approach is applied to Kitty‚Äôs problem. The different panels show the results for different values of a regularization parameter <span class="math inline">\(\lambda\)</span>, that governs the trade-off between achieving the desired label switch and keeping the distance between the original (<span class="math inline">\(\mathbf{x}_i\)</span>) and revised (<span class="math inline">\(\mathbf{x}_i&#39;\)</span>) attributes small. In all but one case, <code>REVISE</code> converges: a decrease in tail length along with an increase in height eventually allows Kitty to cross the decision boundary. In other words, we have successfully turned Kitty into a dog - at least in the eyes of the linear classifier!</p>
<p>We also observe that as we increase <span class="math inline">\(\lambda\)</span> for a fixed learning rate, <code>REVISE</code> takes longer to converge. This should come as no surprise, since higher values of <span class="math inline">\(\lambda\)</span> lead to greater regularization with respect to the penalty we place on the distance that Kitty has to travel. When we penalize too much (<span class="math inline">\(\lambda=10\)</span>), Kitty never reaches the decision boundary, because she is reluctant to change her characteristics beyond a certain point. While not visible to the naked eye, in this particular example <span class="math inline">\(\lambda=0.001\)</span> corresponds to the best choice among the candidate values.</p>
<div class="figure" style="text-align: center"><span id="fig:revise"></span>
<img src="www/revise.gif" alt="The simplified `REVISE` algorithm in action: how Kitty crosses the decision boundary by changing her attributes. Regularization with respect to the distance penalty increases from top left to bottom right."  />
<p class="caption">
Figure 3: The simplified <code>REVISE</code> algorithm in action: how Kitty crosses the decision boundary by changing her attributes. Regularization with respect to the distance penalty increases from top left to bottom right.
</p>
</div>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<p>While hopefully Kitty‚Äôs journey has provided you with some useful intuition, the story is of course very silly. Even if your cat ever seems to signal that she wants to be dog, helping her cross that decision boundary will be very tricky. Some attributes are simply immutable or very difficult to change, which Joshi et al.¬†(2019) do not fail to account for in their approach. Their proposed methodology offers a simple and ingenious approach towards providing individual recourse. Instead of bothering with Black-Box interpretability, why not simply provide a quick remedy in case things go wrong?</p>
<p>That idea certainly has its merit. As this post has hopefully shown, <code>REVISE</code> is easily understood and readily applicable. It could be a very useful tool to provide individual recourse in many real-world applications. As the implementation of our simplified version of <code>REVISE</code> demonstrates, researchers should find it relatively easy to develop the methodology further and tailor it to specific use cases. The simpler version here, for example, may be useful in settings where the dimensionality is relatively small and one can reasonably model the distribution of attributes without the need for generative models.</p>
<p>Still, you may be wondering: if the original classifier is based on poorly defined rules and proxies, then what good does <code>REVISE</code> really do? Going back to the example of high-school teacher Sarah Wysocki, one of the key attributes determining teachers‚Äô evaluations was their students‚Äô performance. Realizing this, some teachers took the shortest route to success by artificially inflating their students‚Äô test scores. That same course of action may well have been suggested by <code>REVISE</code>. As a matter of fact, Joshi et al.¬†(2019) show that this very property of <code>REVISE</code> may be useful in detecting weaknesses of decision making systems before setting them loose (key contribution 3).</p>
<p>Nonetheless, the example of Sarah Wysocki demonstrates that approaches like <code>REVISE</code>, useful as they are, tend to provide solutions for very particular problems. In reality there are many such problems and research on trustworthy AI will therefore need to tackle the issue from various angles. A few places to start include the question of dealing with data that is inherently biased, improving ad-hoc and post-hoc model interpretability and continuing efforts around causality-inspired AI.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-joshi2019towards" class="csl-entry">
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. <span>‚ÄúTowards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.‚Äù</span> <em>arXiv Preprint arXiv:1907.09615</em>.
</div>
<div id="ref-o2016weapons" class="csl-entry">
O‚Äôneil, Cathy. 2016. <em>Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</em>. Crown.
</div>
<div id="ref-ustun2019actionable" class="csl-entry">
Ustun, Berk, Alexander Spangher, and Yang Liu. 2019. <span>‚ÄúActionable Recourse in Linear Classification.‚Äù</span> In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 10‚Äì19.
</div>
</div>
</div>
<div id="annex" class="section level2">
<h2>Annex</h2>
<p>In my blog posts I aim to implement interesting ideas from scratch even if that usually means that things need to undergo some sort of simplification. The benefit of this approach is that experience is educationally very rewarding - both for myself and hopefully my readers. The two sections of this annex show how <code>REVISE</code> and linear classification can be implemented in R.</p>
<div id="revise-simplified" class="section level3">
<h3><code>REVISE</code> (simplified)</h3>
<p>As flagged above, we use a slightly simplified version of algorithm presented in <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span>. In particular, we do not incorporate the threshold on the likelihood, which the authors implement through generative models. We will also pretend here that all attributes are mutable.</p>
<p>Let <span class="math inline">\(y\in\{-1,1\}\)</span> be a binary outcome variable, <span class="math inline">\(X\in\mathbb{R}^d\)</span> a feature matrix containing individuals‚Äô attributes and <span class="math inline">\(g_n(X)\)</span> a corresponding data-dependent classifier. Suppose <span class="math inline">\(y_i=-1\)</span> - the negative outcome - for some individual characterized by attributes <span class="math inline">\(\mathbf{x}_i\)</span>. Then we want to find <span class="math inline">\(\mathbf{x}_i&#39;\)</span> closest to <span class="math inline">\(\mathbf{x}_i\)</span> such that the classifier assigns the positive outcome (primary constraint) and the likelihood of the sample <span class="math inline">\(p(\mathbf{x}_i&#39;)\)</span> exceeds some threshold <span class="math inline">\(\gamma\)</span> (secondary constraint).</p>
<p>In order to identify the smallest set of changes <span class="math inline">\(\delta_i=\mathbf{x}_i^{&#39;}-\mathbf{x}_i\)</span> necessary for individual <span class="math inline">\(i\)</span> to achieve a label switch from <span class="math inline">\(g(\mathbf{x}_i^{&#39;})=1\)</span>, we use gradient descent with Hinge loss <span class="math inline">\(\ell\)</span> to minimize the following function</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \min_{\mathbf{x}_i^{&#39;}}&amp; \ \ell(g_n(\mathbf{x}_i^{&#39;}),1) + \lambda d(\mathbf{x}_i^{&#39;},\mathbf{x}_i) \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(d=||\mathbf{x}_i^{&#39;}-\mathbf{x}_i||\)</span> denotes the Euclidean distance. Note that this time we take the coefficient vector defining <span class="math inline">\(g_n\)</span> as given and instead vary the attributes. In particular, we will perform gradient descent steps as follows</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; {\mathbf{x}_i^{&#39;}}^t&amp;\leftarrow {\mathbf{x}_i^{&#39;}}^{t-1} + \eta \nabla_{{\mathbf{x}_i^{&#39;}}} \left( \ell(g_n(\mathbf{x}_i^{&#39;}),1) + \lambda d(\mathbf{x}_i^{&#39;},\mathbf{x}_i)  \right)  \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate. The descent step is almost equivalent to the one described in <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span>, but here we greatly simplify things by optimizing directly in the attribute space instead of a latent space. The gradient of the loss function looks very similar to <a href="#eq:hinge">(1)</a>. With respect to the Euclidean distance partial derivatives are of the following form:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp;  \frac{\partial ||\mathbf{x}_i^{&#39;}-\mathbf{x}_i||}{\partial {x_i&#39;}^{(d)}}  &amp;= \frac{{x_i&#39;}^{(d)}-{x_i}^{(d)}}{||\mathbf{x}_i^{&#39;}-\mathbf{x}_i||} \\
\end{aligned}
\]</span></p>
<p>The code that implements this optimization follows below.</p>
<pre class="r"><code>#&#39; REVISE algoritm - a simplified version
#&#39;
#&#39; @param classifier The fitted classifier.
#&#39; @param x_star Attributes of individual seeking individual recourse.
#&#39; @param eta Learning rate.
#&#39; @param lambda Regularization parameter.
#&#39; @param n_iter Maximum number of operations.
#&#39; @param save_steps Boolean indicating if intermediate steps should be saved.
#&#39;
#&#39; @return
#&#39; @export
#&#39;
#&#39; @author Patrick Altmeyer
revise.classifier &lt;- function(classifier,x_star,eta=1,lambda=0.01,n_iter=1000,save_steps=FALSE) {
  # Initialization: ----
  d &lt;- length(x_star) # number of dimensions
  if (!is.null(names(x_star))) {
    d_names &lt;- names(x_star) # names of attributes, if provided
  } else {
    d_names &lt;- sprintf(&quot;X%i&quot;, 1:d)
  }
  w &lt;- classifier$coefficients # coefficient vector
  x &lt;- x_star # initialization of revised attributes
  distance &lt;- 0 # initial distance from starting point
  converged &lt;- predict(classifier, newdata = x)[1,1]==1 # positive outcome?
  iter &lt;- 1 # counter
  if (save_steps) {
    steps &lt;- data.table(iter=1, x=x, d=d_names) # save intermediate steps, if desired
  } else {
    steps &lt;- NA
  }
  # Gradients:
  grad &lt;- function(x,y,w) {
    w %*% ifelse(crossprod(x,w) * y&lt;=1,-y,0) # gradient of Hinge loss with respect to X
  }
  grad_dist &lt;- function(x,x_star) {
    d &lt;- length(x_star)
    distance &lt;- dist(matrix(cbind(x_star,x),nrow=d,byrow = T))
    matrix((x-x_star) / distance) # gradient of Euclidean distance with respect to X
  }
  # Gradient descent: ----
  while(!converged &amp; iter&lt;n_iter) {
    if (distance!=0) {
      x &lt;- c(x - eta * (grad(x=matrix(x),y=1,w) + lambda * grad_dist(x,x_star))) # gradient descent step
    } else {
      x &lt;- c(x - eta * grad(x=matrix(x),y=1,w)) # gradient with respect to distance not defined at zero
    }
    converged &lt;- predict(classifier, newdata = x)[1,1]==1 # positive outcome?
    iter &lt;- iter + 1 # update counter
    if (save_steps) {
      steps &lt;- rbind(steps, data.table(iter=iter, x=x, d=d_names))
    }
    distance &lt;- dist(matrix(cbind(x_star,x),nrow=d,byrow = T)) # update distance
  }
  # Output: ----
  if (converged) {
    revise &lt;- x - x_star
  } else {
    revise &lt;- NA
  }
  output &lt;- list(
    x_star = x_star,
    revise = revise,
    classifier = classifier,
    steps = steps,
    lambda = lambda,
    distance = distance,
    mean_distance = mean(abs(revise))
  )
  return(output)
}

revise &lt;- function(classifier,x_star,eta=1,lambda=0.01,n_iter=1000,save_steps=FALSE) {
  UseMethod(&quot;revise&quot;)
}</code></pre>
</div>
<div id="linear-classifier" class="section level3">
<h3>Linear classifier</h3>
<p>Linear classification is implemented through stochastic gradient descent (SGD) under Hinge loss</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \ell(-\mathbf{w}^T\mathbf{x}_i y_i)&amp;=(1-\mathbf{w}^T\mathbf{x}_i y_i)_+ \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> is a coefficient vector, <span class="math inline">\(\mathbf{x}_i\)</span> is the attribute vector of individual <span class="math inline">\(i\)</span> and <span class="math inline">\(y_i\)</span> is individual‚Äôs outcome. Since we apply SGD in order to minimize the loss function <span class="math inline">\(\ell\)</span> by varying <span class="math inline">\(\mathbf{w}\)</span>, we need an expression for its gradient with respect to <span class="math inline">\(\mathbf{w}\)</span>:</p>
<p><span class="math display" id="eq:hinge">\[\begin{equation} 
\begin{aligned}
&amp;&amp; \nabla_{\mathbf{W}} \left( \ell(-\mathbf{w}^T\mathbf{x}_i y_i) \right) &amp;= \begin{cases} -\mathbf{x}_i y_i &amp; \text{if} \ \ \ \mathbf{w}^T\mathbf{x}_i y_i \le 1\\ 0 &amp; \text{otherwise} \end{cases} \\
\end{aligned}
\tag{1}
\end{equation}\]</span></p>
<p>The code below uses this analytical solution to perform SGD over <span class="math inline">\(T\)</span> iterations or as long as updates yield feasible parameter values. As the final vector of coefficients the function returns <span class="math inline">\(\mathbf{\bar{w}}= \frac{1}{T} \sum_{t=1}^{T} \mathbf{w}_t\)</span>. Denoting the optimal coefficient vector as <span class="math inline">\(\mathbf{w}^*\)</span>, it can be shown that under certain conditions <span class="math inline">\(\ell(\mathbf{\bar{w}})\rightarrow\ell(\mathbf{w}^*)\)</span> as <span class="math inline">\(T\rightarrow\infty\)</span>.</p>
<pre class="r"><code>#&#39; Stochastic gradient descent
#&#39;
#&#39; @param X Feature matrix.
#&#39; @param y Vector containing training labels.
#&#39; @param eta Learning rate.
#&#39; @param n_iter Maximum number of iterations.
#&#39; @param w_init Initial parameter values.
#&#39; @param save_steps Boolean checking if coefficients should be saved at each step.
#&#39;
#&#39; @return
#&#39; @export
#&#39;
#&#39; @author Patrick Altmeyer
linear_classifier &lt;- function(X,y,eta=0.001,n_iter=1000,w_init=NULL,save_steps=FALSE) {
  # Initialization: ----
  n &lt;- nrow(X) # number of observations
  d &lt;- ncol(X) # number of dimensions
  if (is.null(w_init)) {
    w &lt;- matrix(rep(0,d)) # initialize coefficients as zero...
  } else {
    w &lt;- matrix(w_init) # ...unless initial values have been provided.
  }
  w_avg &lt;- 1/n_iter * w # initialize average coefficients
  iter &lt;- 1 # iteration count
  if (save_steps) {
    steps &lt;- data.table(iter=0, w=c(w), d=1:d) # if desired, save coefficient at each step
  } else {
    steps &lt;- NA
  }
  feasible_w &lt;- TRUE # to check if coefficients are finite, non-nan, ...
  # Surrogate loss:
  l &lt;- function(X,y,w) {
    x &lt;- (-1) * crossprod(X,w) * y
    pmax(0,1 + x) # Hinge loss
  }
  grad &lt;- function(X,y,w) {
    X %*% ifelse(crossprod(X,w) * y&lt;=1,-y,0) # Gradient of Hinge loss
  }
  # Stochastic gradient descent: ----
  while (feasible_w &amp; iter&lt;n_iter) {
    t &lt;- sample(1:n,1) # random draw
    X_t &lt;- matrix(X[t,])
    y_t &lt;- matrix(y[t])
    v_t &lt;- grad(X_t,y_t,w) # compute estimate of gradient
    # Update:
    w &lt;- w - eta * v_t # update coefficient vector
    feasible_w &lt;- all(sapply(w, function(i) !is.na(i) &amp; is.finite(i))) # check if feasible
    if (feasible_w) {
      w_avg &lt;- w_avg + 1/n_iter * w # update average
    }
    if (save_steps) {
      steps &lt;- rbind(steps, data.table(iter=iter, w=c(w), d=1:d))
    }
    iter &lt;- iter + 1 # increase counter
  }
  # Output: ----
  output &lt;- list(
    X = X,
    y = matrix(y),
    coefficients = w_avg,
    eta = eta,
    n_iter = n_iter,
    steps = steps
  )
  class(output) &lt;- &quot;classifier&quot; # assign S3 class
  return(output)
}

# Methods: ----
print.classifier &lt;- function(classifier) {
  print(&quot;Coefficients:&quot;)
  print(classifier$coefficients)
}
print &lt;- function(classifier) {
  UseMethod(&quot;print&quot;)
}

predict.classifier &lt;- function(classifier, newdata=NULL, discrete=TRUE) {
  if (!is.null(newdata)) {
    fitted &lt;- newdata %*% classifier$coefficients # out-of-sampple prediction
  } else {
    fitted &lt;- classifier$X %*% classifier$coefficients # in-sample fit
  }
  if (discrete) {
    fitted &lt;- sign(fitted) # map to {-1,1}
  }
  return(fitted)
}
predict &lt;- function(classifier, newdata=NULL, discrete=TRUE) {
  UseMethod(&quot;predict&quot;)
}</code></pre>
</div>
</div>
